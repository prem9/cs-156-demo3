{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db18fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import entropy, gmean\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Suppress known numerical warnings (e.g., divide by zero, log of zero)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# --- Parameters ---\n",
    "fs = 50\n",
    "segment_size = 250\n",
    "axes = ['back_x', 'back_y', 'back_z', 'thigh_x', 'thigh_y', 'thigh_z']\n",
    "\n",
    "# --- Load and process all CSV files from harth/ ---\n",
    "folder_path = \"./harth\"\n",
    "features_list = []\n",
    "labels = []\n",
    "\n",
    "def spectral_centroid(freqs, psd):\n",
    "    return np.sum(freqs * psd) / np.sum(psd) if np.sum(psd) > 0 else 0\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "        if 'label' not in df.columns:\n",
    "            continue\n",
    "        for start in range(0, len(df) - segment_size, segment_size):\n",
    "            end = start + segment_size\n",
    "            segment = df.iloc[start:end]\n",
    "            features = {}\n",
    "            for axis in axes:\n",
    "                signal = segment[axis].to_numpy()\n",
    "                fft_vals = fft(signal)\n",
    "                fft_mag = np.abs(fft_vals[:segment_size // 2])\n",
    "                freqs, psd = welch(signal, fs=fs)\n",
    "                psd_sum = np.sum(psd)\n",
    "                psd_norm = psd / psd_sum if psd_sum > 0 else np.zeros_like(psd)\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    features[f'{axis}_spectral_energy'] = np.sum(fft_mag ** 2)\n",
    "                    features[f'{axis}_dominant_freq'] = freqs[np.argmax(np.abs(fft_vals))]\n",
    "                    features[f'{axis}_frequency_variance'] = np.var(psd)\n",
    "                    features[f'{axis}_spectral_centroid'] = spectral_centroid(freqs, psd)\n",
    "                    features[f'{axis}_spectral_entropy'] = entropy(psd_norm)\n",
    "                    features[f'{axis}_spectral_flatness'] = gmean(psd + 1e-12) / (np.mean(psd) + 1e-12)\n",
    "                    features[f'{axis}_peak_freq'] = freqs[np.argmax(psd)]\n",
    "                    features[f'{axis}_bandwidth'] = (\n",
    "                        np.sqrt(np.sum(psd * (freqs - spectral_centroid(freqs, psd))**2) / psd_sum)\n",
    "                        if psd_sum > 0 else 0\n",
    "                    )\n",
    "\n",
    "                for i in range(5):\n",
    "                    features[f'{axis}_fft_coef_{i}'] = fft_mag[i] if i < len(fft_mag) else 0\n",
    "\n",
    "            features_list.append(features)\n",
    "            labels.append(segment['label'].mode()[0])\n",
    "\n",
    "# --- Build dataset ---\n",
    "X = pd.DataFrame(features_list)\n",
    "y = pd.Series(labels)\n",
    "\n",
    "# --- Impute + Split ---\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Define and evaluate models ---\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"Random Forest\": RandomForestClassifier()\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy:\", round(acc, 4))\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    results.append((name, acc))\n",
    "\n",
    "# --- Print Accuracy Summary ---\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Test Accuracy\"]).sort_values(by=\"Test Accuracy\", ascending=False)\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5800cf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Part 3: Model Evaluation on Test Data ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m micro_f1_scores = []\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Part 3: Model Evaluation on Test Data ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels\u001b[49m.items():\n\u001b[32m     13\u001b[39m     y_pred = model.predict(X_test)\n\u001b[32m     14\u001b[39m     acc = accuracy_score(y_test, y_pred)\n",
      "\u001b[31mNameError\u001b[39m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "macro_f1_scores = []\n",
    "micro_f1_scores = []\n",
    "\n",
    "print(\"\\n=== Part 3: Model Evaluation on Test Data ===\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    micro_f1 = f1_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "    macro_f1_scores.append((name, macro_f1))\n",
    "    micro_f1_scores.append((name, micro_f1))\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "    print(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"{name} â€“ Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Classification Report\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# --- Plot Macro and Micro F1 Scores ---\n",
    "macro_df = pd.DataFrame(macro_f1_scores, columns=[\"Model\", \"Macro F1\"])\n",
    "micro_df = pd.DataFrame(micro_f1_scores, columns=[\"Model\", \"Micro F1\"])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(macro_df[\"Model\"], macro_df[\"Macro F1\"], label='Macro F1', alpha=0.7)\n",
    "plt.bar(micro_df[\"Model\"], micro_df[\"Micro F1\"], label='Micro F1', alpha=0.7)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Macro vs. Micro F1 Score by Model\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Optional: K-Fold Cross Validation on Entire Dataset ---\n",
    "print(\"\\n=== Optional: 5-Fold Cross-Validation Accuracy ===\")\n",
    "for name, model in models.items():\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        scores = cross_val_score(model, X_imputed, y, cv=5, scoring='accuracy')\n",
    "        print(f\"{name}: Mean Accuracy = {scores.mean():.4f}, Std = {scores.std():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
